{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780a15bf-0434-4ad1-bc5d-a2ee4a9a6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_fashion.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page\n",
    "url = 'https://www.google.com/search?sca_esv=e03c6cbd519be5e0&sca_upv=1&sxsrf=ADLYWIL8VY3nNlKFwa7T3T8zeyPtmRZIZg:1726565501713&q=gfashion&udm=2&fbs=AEQNm0DPvcmG_nCbmwtBO9j6YBzM68ZanC7g01Skprhw5JoufVCiMv-hxC44jt6JduRQysBab-bgQXjPraaWFXMvOy8Kr1OAG3K-aj3De4zf3-LxKtkBtWaSCp743evHzhY6J0rIQUCXki65vOxhV0cGJtj0S1dF8YREnKrWtJctBkTv8-bs83YpB7p3IMTdYvjisDEty1xSxeLS4B_TKFXUiCrenmEMcA&sa=X&ved=2ahUKEwjpx6Tb1cmIAxV-m2MGHawML5AQtKgLegQIJRAB&biw=1920&bih=869&dpr=1'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Find all image tags\n",
    "image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "\n",
    "# Create a list to hold image URLs\n",
    "image_urls = []\n",
    "\n",
    "# Loop through the image tags and get image URLs\n",
    "for img_tag in image_tags:\n",
    "    img_url = img_tag.get_attribute('src')\n",
    "    if img_url:\n",
    "        image_urls.append(img_url)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_fashion.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850ce1cb-214b-4f77-b66e-e66e7bf83f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_laptops.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page\n",
    "url = 'https://www.google.com/search?q=laptop&sca_esv=e03c6cbd519be5e0&sca_upv=1&udm=2&biw=1920&bih=869&sxsrf=ADLYWIIzjXP_zcQGziZ4qvKtqdmgNb2q4A%3A1726565643242&ei=C03pZt20DrOe4-EPz5PpCQ&ved=0ahUKEwid5uKe1smIAxUzzzgGHc9JOgEQ4dUDCBA&uact=5&oq=laptop&gs_lp=Egxnd3Mtd2l6LXNlcnAiBmxhcHRvcDINEAAYgAQYsQMYQxiKBTINEAAYgAQYsQMYQxiKBTIKEAAYgAQYQxiKBTIIEAAYgAQYsQMyDRAAGIAEGLEDGEMYigUyCBAAGIAEGLEDMggQABiABBixAzIIEAAYgAQYsQMyCBAAGIAEGLEDMggQABiABBixA0j5E1CgDFiREnABeACQAQCYAc4CoAHyCaoBBzAuMy4yLjG4AQPIAQD4AQGYAgegAo0KqAIKwgIHECMYJxjqAsICBBAjGCeYAwmSBwcxLjMuMi4xoAe8IA&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_laptops.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6821bc3-c93c-475d-a696-002d87a437ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_shoes.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"shoes\"\n",
    "url = 'https://www.google.com/search?q=shoes&sca_esv=e03c6cbd519be5e0&sca_upv=1&sxsrf=ADLYWIIQeTB2cFIyWkhD-j-O9WjTFzqmGA:1726566255211&source=hp&biw=1536&bih=695&ei=b0_pZq_SCu-ZseMPibK80Qs&iflsig=AL9hbdgAAAAAZuldf6Docx7wtC0xDYVyuL6Y6HRt4T-O&ved=0ahUKEwjvxcjC2MmIAxXvTGwGHQkZL7oQ4dUDCBA&uact=5&oq=shoes&gs_lp=EgNpbWciBXNob2VzMgQQIxgnMggQABiABBixAzIIEAAYgAQYsQMyCBAAGIAEGLEDMggQABiABBixAzIIEAAYgAQYsQMyCBAAGIAEGLEDMggQABiABBixAzIFEAAYgAQyCBAAGIAEGLEDSKcJUPwDWJ0IcAF4AJABAJgBgwGgAYAFqgEDMC41uAEDyAEA-AEBigILZ3dzLXdpei1pbWeYAgagApYFqAIKwgIHECMYJxjqApgDB5IHAzEuNaAHpx8&sclient=img&udm=2'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_shoes.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d648d91-991d-49e3-a824-222eb9919e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_watches.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"watches\"\n",
    "url = 'https://www.google.com/search?q=watches&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIL0QqQUvVowiRvSVd6ePMcM59mk6Q%3A1726566258663&ei=ck_pZuGWKPmQ4-EP0vbp0AQ&ved=0ahUKEwjhl53E2MmIAxV5yDgGHVJ7GkoQ4dUDCBA&uact=5&oq=watches&gs_lp=Egxnd3Mtd2l6LXNlcnAiB3dhdGNoZXMyDRAAGIAEGLEDGEMYigUyBRAAGIAEMggQABiABBixAzIIEAAYgAQYsQMyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgARI1Q9Q5glYzg9wAXgAkAEAmAGMAqAB0geqAQUwLjUuMbgBA8gBAPgBAZgCB6AC7AeoAgrCAgcQIxgnGOoCwgIEECMYJ8ICChAAGIAEGEMYigXCAhAQABiABBixAxhDGIMBGIoFmAMGkgcFMS41LjGgB6Ig&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_watches.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab3bd75-7d4f-435b-a5d3-30e98d47a65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_shirts.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"shirts\"\n",
    "url = 'https://www.google.com/search?q=shirts&sca_esv=e03c6cbd519be5e0&sca_upv=1&sxsrf=ADLYWIIh3VgAWGIN4IHWjQMpdIccFCRJgA:1726566480522&source=hp&biw=1536&bih=695&ei=UFDpZtvcHIKaseMPoJK7oAs&iflsig=AL9hbdgAAAAAZuleYKpt8DKOmuszE_VYZXJ_qndmg2_p&ved=0ahUKEwjbxP-t2cmIAxUCTWwGHSDJDrQQ4dUDCBA&uact=5&oq=shirts&gs_lp=EgNpbWciBnNoaXJ0czIIEAAYgAQYsQMyBRAAGIAEMggQABiABBixAzIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgARI2TtQzjNYtDhwAXgAkAEAmAH6AaABugeqAQUwLjUuMbgBA8gBAPgBAYoCC2d3cy13aXotaW1nmAIHoALPB6gCCsICBxAjGCcY6gLCAgQQIxgnwgILEAAYgAQYsQMYgwHCAg4QABiABBixAxiDARiKBZgDBZIHBTEuNS4xoAfxHw&sclient=img&udm=2'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_shirts.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022cdc8f-ae77-4979-8428-2e9013c14232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jeans.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"jeans\"\n",
    "url = 'https://www.google.com/search?q=jeans&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWII5KyeV9y9UNs7CC1kXIPz6S6f5fA%3A1726566489878&ei=WVDpZparNeGL4-EPpLOXgAg&ved=0ahUKEwjWu72y2cmIAxXhxTgGHaTZBYAQ4dUDCBA&uact=5&oq=jeans&gs_lp=Egxnd3Mtd2l6LXNlcnAiBWplYW5zMg0QABiABBixAxhDGIoFMgoQABiABBhDGIoFMg0QABiABBixAxhDGIoFMggQABiABBixAzIIEAAYgAQYsQMyChAAGIAEGEMYigUyCBAAGIAEGLEDMgoQABiABBhDGIoFMg0QABiABBixAxhDGIoFMgUQABiABEiiHFDGFVjIGXABeACQAQGYAawCoAHiBqoBBzAuNC4wLjG4AQPIAQD4AQGYAgWgAscEqAIKwgIHECMYJxjqApgDB5IHAzEuNKAHpBs&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jeans.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5dc38a2-2106-43a2-8bd3-9a2c97ba3109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_skirts.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"skirts\"\n",
    "url = 'https://www.google.com/search?q=skirts&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIJsgjD1tl4gOqKf7U3NG0fowS4lhg%3A1726566558614&ei=nlDpZsKaJeTH4-EP0oDGmA8&ved=0ahUKEwjC4aDT2cmIAxXk4zgGHVKAEfMQ4dUDCBA&uact=5&oq=skirts&gs_lp=Egxnd3Mtd2l6LXNlcnAiBnNraXJ0czIIEAAYgAQYsQMyDRAAGIAEGLEDGEMYigUyCBAAGIAEGLEDMgoQABiABBhDGIoFMgUQABiABDIKEAAYgAQYQxiKBTIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgARI-T9Q0zRYxDlwAXgAkAEAmAGRAqAB6QeqAQUwLjUuMbgBA8gBAPgBAZgCB6AC_weoAgrCAgcQIxgnGOoCwgIEECMYJ5gDBpIHBTEuNS4xoAeuHw&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_skirts.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed2d6c6-47f5-4108-abe3-814c99b61b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_tops.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"Tops\"\n",
    "url = 'https://www.google.com/search?q=Tops&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIJZQz_4eDIkhLi_ThIIIn4Yti9x5g%3A1726566625186&ei=4VDpZomAC_Hy4-EPlraM6AM&ved=0ahUKEwjJ9P_y2cmIAxVx-TgGHRYbAz0Q4dUDCBA&uact=5&oq=Tops&gs_lp=Egxnd3Mtd2l6LXNlcnAiBFRvcHMyDRAAGIAEGLEDGEMYigUyCBAAGIAEGLEDMgoQABiABBhDGIoFMgoQABiABBhDGIoFMggQABiABBixAzIKEAAYgAQYQxiKBTIIEAAYgAQYsQMyChAAGIAEGEMYigUyChAAGIAEGEMYigUyChAAGIAEGEMYigVIzhtQ0A5Y_hFwAXgAkAEAmAGMAqABrAWqAQUwLjMuMbgBA8gBAPgBAZgCBaACvQWoAgrCAgcQIxgnGOoCwgIFEAAYgASYAwiSBwUxLjMuMaAHzhU&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_tops.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f418aac0-6d7a-419f-adf7-d2b759bebfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_sandals_slippers.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"sandals and slippers\"\n",
    "url = 'https://www.google.com/search?q=sandals+and+slippers&sca_esv=e03c6cbd519be5e0&sca_upv=1&sxsrf=ADLYWIKkHgCIttaJCZY8lW-Q6QYtlseSNw:1726566831314&source=hp&biw=1536&bih=695&ei=r1HpZrH4EIOmseMPrJXdSA&iflsig=AL9hbdgAAAAAZulfvzA4c9_dZ5CIO7L_nLaDw3KnTl5F&ved=0ahUKEwjxi6PV2smIAxUDU2wGHaxKFwkQ4dUDCBA&uact=5&oq=sandals+and+slippers&gs_lp=EgNpbWciFHNhbmRhbHMgYW5kIHNsaXBwZXJzMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIGEAAYBRgeMgYQABgFGB4yBhAAGAUYHjIGEAAYBRgeMgYQABgFGB4yBhAAGAUYHkjAUlCVLlicUXADeACQAQCYAfYBoAHmGKoBBjAuMjEuMbgBA8gBAPgBAYoCC2d3cy13aXotaW1nmAIYoAKcGKgCCsICBxAjGCcY6gLCAgQQIxgnwgIIEAAYgAQYsQPCAg4QABiABBixAxiDARiKBZgDBpIHBjMuMjAuMaAH8nc&sclient=img&udm=2'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_sandals_slippers.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d004110e-6e28-48d4-8e0d-f0f5d208a4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_perfumes.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"perfumes\"\n",
    "url = 'https://www.google.com/search?q=perfumes&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIKPIanCNpIh5lVrDbxUD4BTzF4vJA%3A1726566843327&ei=u1HpZrvPE4WR4-EPk8y9sAk&ved=0ahUKEwj7mILb2smIAxWFyDgGHRNmD5YQ4dUDCBA&uact=5&oq=perfumes&gs_lp=Egxnd3Mtd2l6LXNlcnAiCHBlcmZ1bWVzMggQABiABBixAzIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEjXHFDJEljsGnABeACQAQCYAYICoAH5CaoBBTAuNy4xuAEDyAEA-AEBmAIJoAKXCqgCCsICBxAjGCcY6gLCAgQQIxgnwgILEAAYgAQYsQMYgwHCAgoQABiABBhDGIoFwgIOEAAYgAQYsQMYgwEYigXCAg0QABiABBixAxhDGIoFwgIEEAAYA5gDB5IHBTEuNy4xoAfCKw&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_perfumes.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "502364bd-232d-4366-95aa-633a0906f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_slippers.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"slippers\"\n",
    "url = 'https://www.google.com/search?q=slippers&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIJ18IPB1iPfZxv_fYihycCYHqNrbw%3A1726566896563&ei=8FHpZq2FIumI4-EP9pjnyA0&ved=0ahUKEwitvbP02smIAxVpxDgGHXbMGdkQ4dUDCBA&uact=5&oq=slippers&gs_lp=Egxnd3Mtd2l6LXNlcnAiCHNsaXBwZXJzMggQABiABBixAzIIEAAYgAQYsQMyCBAAGIAEGLEDMggQABiABBixAzIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEiUGFCjB1jiEXABeACQAQCYAZQCoAH1CaoBBTAuNy4xuAEDyAEA-AEBmAIJoAKTCqgCCsICBxAjGCcY6gLCAgQQIxgnwgIKEAAYgAQYQxiKBcICCxAAGIAEGLEDGIMBwgINEAAYgAQYsQMYQxiKBZgDCJIHBTEuNy4xoAfyKg&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_slippers.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d7060c-265f-4261-be01-2f344b092b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_belts.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"belts\"\n",
    "url = 'https://www.google.com/search?q=belts&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIKHAL-BMwBX0IuqaijsPPM4ZVojUQ%3A1726566951654&ei=J1LpZt_OJ72r4-EPgcS5mA4&ved=0ahUKEwif_tWO28mIAxW91TgGHQFiDuMQ4dUDCBA&uact=5&oq=belts&gs_lp=Egxnd3Mtd2l6LXNlcnAiBWJlbHRzMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEjqEVCHCVjWDnABeACQAQCYAZECoAHuBqoBBTAuNC4xuAEDyAEA-AEBmAIGoAKBB6gCCsICBxAjGCcY6gLCAgQQIxgnwgIKEAAYgAQYQxiKBcICDRAAGIAEGLEDGEMYigXCAggQABiABBixA5gDBpIHBTEuNC4xoAf_HQ&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, min_images=400, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        # Exit loop if we have enough images\n",
    "        if len(image_urls) >= min_images:\n",
    "            break\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Ensure we have at least the minimum number of images\n",
    "if len(image_urls) < 400:\n",
    "    print(\"Warning: Less than 400 images found.\")\n",
    "else:\n",
    "    # Save image URLs to CSV\n",
    "    output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_belts.csv'\n",
    "    df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0d7dda8-dc69-4acf-a448-328ab48789c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Less than 400 images found.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"jewellery female\"\n",
    "url = 'https://www.google.com/search?q=jewellery+female&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWILff0-rjSNjvqj_iEWtCW00VbVi8A%3A1726567099293&ei=u1LpZtzEEb2U4-EP8Yz-oAM&ved=0ahUKEwicjonV28mIAxU9yjgGHXGGHzQQ4dUDCBA&uact=5&oq=jewellery+female&gs_lp=Egxnd3Mtd2l6LXNlcnAiEGpld2VsbGVyeSBmZW1hbGUyBRAAGIAEMgUQABiABDIFEAAYgAQyBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeMgYQABgIGB5IqBlQ9wRY4hdwAngAkAEAmAGyAaAB0wmqAQMwLji4AQPIAQD4AQGYAgqgAvEJwgINEAAYgAQYsQMYQxiKBcICCBAAGIAEGLEDwgIQEAAYgAQYsQMYQxiDARiKBcICCxAAGIAEGLEDGIMBwgIJEAAYgAQYGBgKwgIKEAAYgAQYQxiKBcICBhAAGAUYHpgDAIgGAZIHAzIuOKAH9C0&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, min_images=400, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        # Exit loop if we have enough images\n",
    "        if len(image_urls) >= min_images:\n",
    "            break\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Ensure we have at least the minimum number of images\n",
    "if len(image_urls) < 400:\n",
    "    print(\"Warning: Less than 400 images found.\")\n",
    "else:\n",
    "    # Save image URLs to CSV\n",
    "    output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jewellery_female.csv'\n",
    "    df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a57d13a3-739f-450c-bcc8-ed1667d049e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_necklace.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"necklace\"\n",
    "url = 'https://www.google.com/search?q=necklace&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIJJXD_CsmnFUGtebEUhw_g9jD0PMQ%3A1726567112686&ei=yFLpZuS6KdGq4-EPs-ue4AQ&ved=0ahUKEwjkvrrb28mIAxVR1TgGHbO1B0wQ4dUDCBA&uact=5&oq=necklace&gs_lp=Egxnd3Mtd2l6LXNlcnAiCG5lY2tsYWNlMg0QABiABBixAxhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMg0QABiABBixAxhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFSMgbUNkQWLYZcAF4AJABAJgB1gGgAYYLqgEFMC43LjG4AQPIAQD4AQGYAgmgAqMLqAIKwgIHECMYJxjqAsICBBAjGCfCAgsQABiABBixAxiDAcICDhAAGIAEGLEDGIMBGIoFwgIEEAAYA8ICBRAAGIAEwgIIEAAYgAQYsQOYAwaSBwUxLjYuMqAH0TI&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, min_images=400, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        # Exit loop if we have enough images\n",
    "        if len(image_urls) >= min_images:\n",
    "            break\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Ensure we have at least the minimum number of images\n",
    "if len(image_urls) < 400:\n",
    "    print(\"Warning: Less than 400 images found.\")\n",
    "else:\n",
    "    # Save image URLs to CSV\n",
    "    output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_necklace.csv'\n",
    "    df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3c684fb-2965-413a-aace-3e9ed0f0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_rings.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"rings\"\n",
    "url = 'https://www.google.com/search?q=rings&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIKSbsIggKOFMqXzQ4NLjn58iKpvWQ%3A1726567132954&ei=3FLpZu76OePj4-EP1OaAsQs&ved=0ahUKEwju2I_l28mIAxXj8TgGHVQzILYQ4dUDCBA&uact=5&oq=rings&gs_lp=Egxnd3Mtd2l6LXNlcnAiBXJpbmdzMg0QABiABBixAxhDGIoFMg0QABiABBixAxhDGIoFMg0QABiABBixAxhDGIoFMg0QABiABBixAxhDGIoFMgoQABiABBhDGIoFMgUQABiABDIKEAAYgAQYQxiKBTIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDiUGFCjB1jiEXABeACQAQCYAZQCoAH1CaoBBTAuNy4xuAEDyAEA-AEBmAIJoAKTCqgCCsICBxAjGCcY6gLCAgQQIxgnwgIKEAAYgAQYQxiKBcICCxAAGIAEGLEDGIMBwgINEAAYgAQYsQMYQxiKBZgDCJIHBTEuNy4xoAfyKg&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, min_images=400, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        # Exit loop if we have enough images\n",
    "        if len(image_urls) >= min_images:\n",
    "            break\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Ensure we have at least the minimum number of images\n",
    "if len(image_urls) < 400:\n",
    "    print(\"Warning: Less than 400 images found.\")\n",
    "else:\n",
    "    # Save image URLs to CSV\n",
    "    output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_rings.csv'\n",
    "    df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9e9ede9-285a-45c6-a234-60582a772973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jackets.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"jackets\"\n",
    "url = 'https://www.google.com/search?q=jackets&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIJuiBVf4Q86SMMoOgXuIZu_mRasJg%3A1726567210164&ei=KlPpZo_ZCeOX4-EPyZq_yQo&ved=0ahUKEwiPlviJ3MmIAxXjyzgGHUnNL6kQ4dUDCBA&uact=5&oq=jackets&gs_lp=Egxnd3Mtd2l6LXNlcnAiB2phY2tldHMyCBAAGIAEGLEDMgUQABiABDIIEAAYgAQYsQMyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAESNcQUOUGWIwPcAF4AJABAJgBnAKgAcsJqgEFMC42LjG4AQPIAQD4AQGYAgigAuMJqAIKwgIHECMYJxjqAsICBBAjGCfCAgoQABiABBhDGIoFwgINEAAYgAQYsQMYQxiKBZgDB-IDBRIBMSBAkgcFMS42LjGgB9cn&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, min_images=400, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        # Exit loop if we have enough images\n",
    "        if len(image_urls) >= min_images:\n",
    "            break\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Ensure we have at least the minimum number of images\n",
    "if len(image_urls) < 400:\n",
    "    print(\"Warning: Less than 400 images found.\")\n",
    "else:\n",
    "    # Save image URLs to CSV\n",
    "    output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jackets.csv'\n",
    "    df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2bc431e-805c-4957-a4d1-551ee1e3b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been saved to C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jewellery.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "chrome_driver_path = r'C:\\\\Users\\\\madhu\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe'\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the URL of the Google search results page for \"jewellery\"\n",
    "url = 'https://www.google.com/search?q=jewellery&sca_esv=e03c6cbd519be5e0&sca_upv=1&biw=1920&bih=869&udm=2&sxsrf=ADLYWIJ5XlYraeuFSpAyWrVzMB2pJhDjow%3A1726567448069&ei=GFTpZvnlA4SS4-EP7p7a0AQ&ved=0ahUKEwj50bD73MmIAxUEyTgGHW6PFkoQ4dUDCBA&uact=5&oq=jewellery&gs_lp=Egxnd3Mtd2l6LXNlcnAiCWpld2VsbGVyeTIEECMYJzIEECMYJzIIEAAYgAQYsQMyCBAAGIAEGLEDMggQABiABBixAzILEAAYgAQYsQMYgwEyCxAAGIAEGLEDGIMBMggQABiABBixAzIFEAAYgAQyCBAAGIAEGLEDSMcaULIRWNMScAF4AJABAJgByAKgAfMDqgEHMC4xLjAuMbgBA8gBAPgBAZgCAqACswGYAwDiAwUSATEgQIgGAZIHAzEuMaAHgQw&sclient=gws-wiz-serp'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Function to scroll and load more images\n",
    "def scroll_and_load_images(driver, max_images=1000):\n",
    "    image_urls = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while len(image_urls) < max_images:\n",
    "        # Find all image tags\n",
    "        image_tags = driver.find_elements(By.XPATH, '//img')\n",
    "        \n",
    "        # Loop through the image tags and get image URLs\n",
    "        for img_tag in image_tags:\n",
    "            img_url = img_tag.get_attribute('src')\n",
    "            if img_url and img_url not in image_urls:\n",
    "                image_urls.add(img_url)\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new images to load\n",
    "        \n",
    "        # Check if we have reached the end of the page\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        # Exit loop if we have enough images\n",
    "        if len(image_urls) >= max_images:\n",
    "            break\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "# Get image URLs\n",
    "image_urls = scroll_and_load_images(driver)\n",
    "\n",
    "# Save image URLs to CSV\n",
    "output_file_path = r'C:\\\\Users\\\\madhu\\\\JUPYTER PYTHON\\\\web scarping tutooorial\\\\image_urls_jewellery.csv'\n",
    "df = pd.DataFrame(image_urls, columns=['Image_URL'])\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"Image URLs have been saved to {output_file_path}\")\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d9c38-b131-4238-a5ec-6f069e6a0559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
